{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d47f493b-f381-48b2-b44d-13e99db6a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained Word2Vec model...\n",
      "[--------------------------------------------------] 1.4% 23.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===-----------------------------------------------] 7.2% 119.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======--------------------------------------------] 12.9% 214.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========-----------------------------------------] 18.9% 313.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============--------------------------------------] 25.1% 417.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============-----------------------------------] 30.9% 514.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================--------------------------------] 37.1% 616.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================-----------------------------] 43.2% 718.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================--------------------------] 49.0% 815.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================-----------------------] 55.0% 914.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 61.4% 1021.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================----------------] 68.0% 1130.7/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================================-------------] 74.2% 1234.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================----------] 80.6% 1340.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================================--------] 85.7% 1424.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================-----] 91.0% 1513.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================================---] 95.2% 1583.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 98.8% 1643.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded.\n",
      "\n",
      "Top 5 words similar to 'music':\n",
      "  classical_music: 0.7198\n",
      "  jazz: 0.6835\n",
      "  Music: 0.6596\n",
      "  Without_Donny_Kirshner: 0.6416\n",
      "  songs: 0.6396\n",
      "\n",
      "Top 5 words similar to 'market':\n",
      "  markets: 0.7666\n",
      "  marketplace: 0.6955\n",
      "  themarket: 0.5991\n",
      "  maket: 0.5921\n",
      "  mkt: 0.5859\n",
      "\n",
      "Top 5 words similar to 'city':\n",
      "  citys: 0.6804\n",
      "  mayor: 0.6751\n",
      "  town: 0.6724\n",
      "  municipality: 0.6531\n",
      "  municipal: 0.6223\n",
      "\n",
      "Top 5 words similar to 'computer':\n",
      "  computers: 0.7979\n",
      "  laptop: 0.6640\n",
      "  laptop_computer: 0.6549\n",
      "  Computer: 0.6473\n",
      "  com_puter: 0.6082\n",
      "\n",
      "Top 5 words similar to 'happy':\n",
      "  glad: 0.7409\n",
      "  pleased: 0.6632\n",
      "  ecstatic: 0.6627\n",
      "  overjoyed: 0.6599\n",
      "  thrilled: 0.6514\n",
      "\n",
      "Analogy: ['woman', 'king'] minus ['man'] -> Top matches:\n",
      "  queen: 0.7118\n",
      "  monarch: 0.6190\n",
      "  princess: 0.5902\n",
      "  crown_prince: 0.5499\n",
      "  prince: 0.5377\n",
      "\n",
      "Analogy: ['girl', 'brother'] minus ['sister'] -> Top matches:\n",
      "  boy: 0.8510\n",
      "  teenager: 0.6659\n",
      "  man: 0.6658\n",
      "  teenage_girl: 0.6196\n",
      "  son: 0.6025\n",
      "\n",
      "Analogy: ['Paris', 'Germany'] minus ['France'] -> Top matches:\n",
      "  Berlin: 0.7644\n",
      "  Frankfurt: 0.7330\n",
      "  Dusseldorf: 0.7009\n",
      "  Munich: 0.6774\n",
      "  Cologne: 0.6470\n",
      "Cosine similarity between 'king - man + woman' and 'queen': 0.7301\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pretrained Word2Vec model (Google News, 300‑dimensional, ~3M tokens)\n",
    "print(\"Loading pretrained Word2Vec model...\")\n",
    "wv = api.load(\"word2vec-google-news-300\")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Part 1.A: Pick 5 words and display their top-5 similar words\n",
    "words = [\"music\", \"market\", \"city\", \"computer\", \"happy\"]\n",
    "for w in words:\n",
    "    print(f\"\\nTop 5 words similar to '{w}':\")\n",
    "    try:\n",
    "        for sim_word, score in wv.most_similar(positive=[w], topn=5):\n",
    "            print(f\"  {sim_word}: {score:.4f}\")\n",
    "    except KeyError:\n",
    "        print(f\"  Word '{w}' not in vocabulary.\")\n",
    "\n",
    "# Part 1.B: Analogy experiments (vector arithmetic)\n",
    "analogies = [\n",
    "    ([\"woman\", \"king\"], [\"man\"]),        # king - man + woman\n",
    "    ([\"girl\", \"brother\"], [\"sister\"]),   # brother - sister analog\n",
    "    ([\"Paris\", \"Germany\"], [\"France\"]),  # Paris - France + Germany ≈ Berlin\n",
    "]\n",
    "for positive, negative in analogies:\n",
    "    try:\n",
    "        result = wv.most_similar(positive=positive, negative=negative, topn=5)\n",
    "        print(f\"\\nAnalogy: {positive} minus {negative} -> Top matches:\")\n",
    "        for word, score in result:\n",
    "            print(f\"  {word}: {score:.4f}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"  Analogy skipped due to missing word: {e}\")\n",
    "\n",
    "# Additionally compute cosine similarity for verification\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def vector_analogy(a, b, c, target):\n",
    "    # compute a - b + c\n",
    "    vec = wv[a] - wv[b] + wv[c]\n",
    "    sim = cosine_similarity(vec.reshape(1, -1), wv[target].reshape(1, -1))[0,0]\n",
    "    print(f\"Cosine similarity between '{a} - {b} + {c}' and '{target}': {sim:.4f}\")\n",
    "\n",
    "# Compute similarity for the main analogy\n",
    "vector_analogy(\"king\", \"man\", \"woman\", \"queen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3217ccb0-2e8a-4354-b0a6-3408acbd2eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dhanvidoshi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dhanvidoshi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dhanvidoshi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. NLTK Downloads\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r'<.*?>', '', text)                # remove HTML\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)     # remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text).lower()   # keep alphabets only\n",
    "    tokens = word_tokenize(text)\n",
    "    return [lemm.lemmatize(tok) for tok in tokens if tok not in stop and len(tok) > 1]\n",
    "\n",
    "# 2. Load IMDB dataset (rename to IMDB_Dataset.csv)\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df['tokens'] = df['review'].apply(clean_and_tokenize)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768518bc-bf61-4ee1-a29f-faa4f1679095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic EDA: class balance\n",
    "print(\"Label distribution:\", df['sentiment'].value_counts())\n",
    "\n",
    "# 3. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['tokens'], df['sentiment'], test_size=0.2, random_state=42, stratify=df['sentiment']\n",
    ")\n",
    "\n",
    "# 4. Train custom embedding models\n",
    "sg_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=5, sg=1, workers=4)\n",
    "cbow_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=5, sg=0, workers=4)\n",
    "ft_model = FastText(sentences=X_train, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Pretrained Word2Vec\n",
    "pretrained = api.load(\"word2vec-google-news-300\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63ad84af-282e-4610-9ace-c1b84072a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Vector Averaging Function\n",
    "def avg_vec(tokens, model, dim):\n",
    "    vecs = [model[w] for w in tokens if w in model]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(dim)\n",
    "\n",
    "def build_matrix(tokens_list, model, dim):\n",
    "    return np.array([avg_vec(toks, model, dim) for toks in tokens_list])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e3e4832-cff4-4a42-9851-d67f2520845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['negative', 'positive', 'accuracy', 'macro avg', 'weighted avg'])\n"
     ]
    }
   ],
   "source": [
    "rep = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
    "print(rep.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c72799d-4f41-4c8a-9578-8ee18e4b7332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys in report: dict_keys(['negative', 'positive', 'accuracy', 'macro avg', 'weighted avg'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rep = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
    "print(\"Available keys in report:\", rep.keys())\n",
    "\n",
    "# Suppose labels are lowercase 'negative' and 'positive'\n",
    "f1_pos = rep['positive']['f1-score']\n",
    "precision_pos = rep['positive']['precision']\n",
    "recall_pos = rep['positive']['recall']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9ff1af9-55c4-4313-8699-5d2ba1e285c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train and Evaluate Classifiers\n",
    "results = []\n",
    "for name, model, dim in [\n",
    "    (\"Pretrained Word2Vec\", pretrained, 300),\n",
    "    (\"Custom Skip-Gram\", sg_model.wv, 100),\n",
    "    (\"Custom CBOW\", cbow_model.wv, 100),\n",
    "    (\"Custom FastText\", ft_model.wv, 100)\n",
    "]:\n",
    "    Xtr = build_matrix(X_train, model, dim)\n",
    "    Xte = build_matrix(X_test, model, dim)\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(Xtr, y_train)\n",
    "    preds = clf.predict(Xte)\n",
    "\n",
    "    rep = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
    "\n",
    "    # See what labels are present: e.g. rep.keys() -> dict_keys(['negative', 'positive', 'accuracy', 'macro avg', 'weighted avg'])\n",
    "    label_names = [lbl for lbl in rep.keys() if lbl not in ('accuracy', 'macro avg', 'weighted avg')]\n",
    "    cls = label_names[0] if label_names else None  # typically 'positive' or 'Negative', etc.\n",
    "\n",
    "    if cls:\n",
    "        prec = rep[cls]['precision']\n",
    "        rec = rep[cls]['recall']\n",
    "        f1 = rep[cls]['f1-score']\n",
    "    else:\n",
    "        prec = rec = f1 = None\n",
    "\n",
    "    acc = rep.get('accuracy', accuracy_score(y_test, preds))\n",
    "    results.append([name, acc, prec, rec, f1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1caed-aa60-4579-bd2d-549797dbe43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Create a results DataFrame\n",
    "res_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "print(res_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
